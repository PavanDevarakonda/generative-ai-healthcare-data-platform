{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65d2a9-c9d8-4a3c-a5a5-5e1634a13eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets peft trl transformers pandas torch spacy nltk rouge_score bert_score sentence_transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fb053-9de5-472f-a6fd-aae16b7f6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1009f-873c-4906-a9e5-07cfc9c0cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(instruction, input_text):\n",
    "    \"\"\"Format the instruction and input into a prompt\"\"\"\n",
    "    if input_text:\n",
    "        return f\"{instruction}\\n\\n{input_text}\"\n",
    "    return instruction\n",
    "\n",
    "def load_and_format_dataset(file_path, train_split=0.8, output_dir=\"data\"):\n",
    "    \"\"\"Improved dataset preparation\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Validate and filter\n",
    "    required_columns = [\"instruction\", \"input\", \"output\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Dataset must contain {required_columns} columns\")\n",
    "    df = df[df[\"input\"] != \"No structured clinical data available.\"]\n",
    "\n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create chat format\n",
    "        user_msg = create_prompt(row[\"instruction\"], row[\"input\"])\n",
    "        assistant_msg = row[\"output\"]\n",
    "\n",
    "        # Create both formats\n",
    "        formatted_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "            ],\n",
    "            \"text\": f\"### User: {user_msg} ###\\n### Assistant: {assistant_msg} ###\"\n",
    "        })\n",
    "\n",
    "    # Split and save\n",
    "    train_size = int(len(formatted_data) * train_split)\n",
    "    for split, data in [(\"train\", formatted_data[:train_size]),\n",
    "                       (\"validation\", formatted_data[train_size:])]:\n",
    "        with open(os.path.join(output_dir, f\"{split}.jsonl\"), \"w\") as f:\n",
    "            for item in data:\n",
    "                json.dump(item, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Saved {train_size} training and {len(formatted_data)-train_size} validation examples\")\n",
    "    return load_dataset(\"json\", data_files={\n",
    "        \"train\": os.path.join(output_dir, \"train.jsonl\"),\n",
    "        \"validation\": os.path.join(output_dir, \"validation.jsonl\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbbbc2-616e-430e-a6da-842e8b3ba33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_dataset(dataset, tokenizer, output_dir=\"preprocessed_data\"):\n",
    "    \"\"\"Pre-tokenize and cache dataset\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=512)  # Truncate to 512 tokens\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"messages\", \"text\"])\n",
    "    tokenized_dataset.save_to_disk(output_dir)\n",
    "    return tokenized_dataset\n",
    "\n",
    "def configure_qlora_model(model_name=\"BioMistral/BioMistral-7B\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "        llm_int8_skip_modules=[\"lm_head\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"offload\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,  # Reduced from 8\n",
    "        lora_alpha=8,  # Reduced from 16\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        inference_mode=False,\n",
    "        fan_in_fan_out=False,\n",
    "        modules_to_save=[\"embed_tokens\", \"lm_head\"]\n",
    "    )\n",
    "\n",
    "    print(\"Applying PEFT adapters to the model...\")\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    print(f\"[DEBUG] Type after get_peft_model: {type(peft_model)}\")\n",
    "\n",
    "    if not isinstance(peft_model, (PeftModel, PeftModelForCausalLM)):\n",
    "        raise ValueError(\"Model is not a PEFT model instance!\")\n",
    "    else:\n",
    "        print(\"[OK] Model wrapped with PEFT successfully.\")\n",
    "\n",
    "    print(peft_model.print_trainable_parameters())\n",
    "\n",
    "    for name, param in peft_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return peft_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897ae90-7d4b-4ba1-acb1-bd405582debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer(model, dataset, output_dir=\"biomistral_finetuned\"):\n",
    "    if not isinstance(model, (PeftModel, PeftModelForCausalLM)):\n",
    "        raise ValueError(\"Model is not a PEFT-wrapped instance! Cannot continue with training.\")\n",
    "\n",
    "    print(f\"Model is a PEFT model: {isinstance(model, (PeftModel, PeftModelForCausalLM))}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,  # Increased from 1\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,  # Reduced from 8\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        bf16=True,  # Changed from fp16=True\n",
    "        bf16_full_eval=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_steps=500,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "    )\n",
    "\n",
    "    def formatting_func(example):\n",
    "        return \"\\n\".join([\n",
    "            f\"### {msg['role'].capitalize()}: {msg['content']} ###\"\n",
    "            for msg in example[\"messages\"]\n",
    "        ])\n",
    "\n",
    "    print(\"Creating SFTTrainer...\")\n",
    "    return SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        args=training_args,\n",
    "        formatting_func=formatting_func,\n",
    "        peft_config=None,\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory\n",
    "    print(\"Loading and preparing dataset...\")\n",
    "    dataset = load_and_format_dataset(\"bio_mistral_qa_combined.csv\")\n",
    "    print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "    print(\"Preprocessing and tokenizing dataset...\")\n",
    "    model, tokenizer = configure_qlora_model()  # Load tokenizer first\n",
    "    dataset = preprocess_and_save_dataset(dataset, tokenizer)\n",
    "    print(f\"Preprocessed dataset: {dataset}\")\n",
    "\n",
    "    print(\"Configuring QLoRA model...\")\n",
    "    model, tokenizer = configure_qlora_model()  # Reload model\n",
    "    print(\"QLoRA model configured successfully!\")\n",
    "\n",
    "    if not isinstance(model, (PeftModel, PeftModelForCausalLM)):\n",
    "        raise ValueError(\"Model is not properly wrapped as a PEFT model!\")\n",
    "\n",
    "    print(\"Setting up trainer...\")\n",
    "    trainer = setup_trainer(model, dataset)\n",
    "    print(\"Trainer configured successfully!\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model()\n",
    "    print(f\"Model trained and saved to {trainer.args.output_dir}\")\n",
    "\n",
    "    return model, tokenizer, trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer, trainer = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621d63f-90a4-48da-b6ea-764b5405bfbf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5e156-76e3-44c5-873b-9ec6cfd95767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch pandas numpy scikit-learn rouge-score nltk scispacy\n",
    "\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed50e7-c8e0-4aeb-81c3-9f681b0e595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e5c81-6c51-477e-97b2-1d1eacf15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy thinc spacy scispacy\n",
    "!pip install numpy==1.26.4\n",
    "!pip install spacy==3.7.2\n",
    "!pip install scispacy==0.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f275f9e-89a8-4f2b-af25-db988a6405a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5434a68-1a86-410a-a912-5d1dd35939fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch sentence-transformers spacy nltk pandas rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ce288-6eea-46ad-be54-3ed55a683cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate bitsandbytes transformers bert-score peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42e323-f46f-42d1-80b7-e8e017efa2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import os\n",
    "\n",
    "# Suppress transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51e00c-1124-4433-9517-af92c1deda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy and SentenceTransformer models\n",
    "def load_spacy_model():\n",
    "    \"\"\"Load spaCy medical NER model.\"\"\"\n",
    "    try:\n",
    "        return spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading spaCy model: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_sentence_transformer():\n",
    "    \"\"\"Load SentenceTransformer for FCS.\"\"\"\n",
    "    try:\n",
    "        return SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer: {e}\")\n",
    "        return None\n",
    "\n",
    "nlp = load_spacy_model()\n",
    "embedder = load_sentence_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b8b62-8fae-4495-9eb1-10b0d709c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "def load_fine_tuned_model(model_name=\"BioMistral/BioMistral-7B\", checkpoint_dir=\"biomistral_finetuned\"):\n",
    "    \"\"\"Load the fine-tuned QLoRA model and tokenizer.\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "            llm_int8_skip_modules=[\"lm_head\"]\n",
    "        )\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            offload_folder=\"offload\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            offload_state_dict=True\n",
    "        )\n",
    "        base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            if os.path.exists(os.path.join(checkpoint_dir, \"adapter_model.bin\")):\n",
    "                print(f\"Loading fine-tuned model from {checkpoint_dir}\")\n",
    "                model = PeftModel.from_pretrained(base_model, checkpoint_dir, is_trainable=False)\n",
    "            else:\n",
    "                checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "                if checkpoints:\n",
    "                    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "                    print(f\"Loading fine-tuned model from checkpoint: {checkpoint_path}\")\n",
    "                    model = PeftModel.from_pretrained(base_model, checkpoint_path, is_trainable=False)\n",
    "                else:\n",
    "                    raise ValueError(f\"No checkpoints or final model found in {checkpoint_dir}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model, tokenizer = load_fine_tuned_model()\n",
    "if model is None or tokenizer is None:\n",
    "    raise ValueError(\"Failed to load fine-tuned model or tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9244103-6fa2-4386-a4d2-4fd87e5b3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [\n",
    "    \"Are there any further procedures planned for the patient?\",\n",
    "    \"Does the patient require long term monitoring?\",\n",
    "    \"What precautions does the patient need to take post-discharge?\",\n",
    "    \"What medications is the patient currently taking?\",\n",
    "    \"What is the patient's primary diagnosis?\"\n",
    "]\n",
    "\n",
    "inputs = [\n",
    "    \"Gender: F\\nChief Complaint: Abdominal distention, nausea, and vomiting\\nHistory: Cirrhosis, multiple paracenteses for ascites\\nPlan: Schedule regular paracentesis every 2 weeks\",\n",
    "    \"Gender: F\\nChief Complaint: Abdominal distention, nausea, and vomiting\\nPlan: Monitor weight and abdominal girth daily; assess for signs of fluid overload\",\n",
    "    \"Gender: M\\nChief Complaint: Abd pain, Hypotension\\nDischarge Plan: Follow low sodium diet, take prescribed meds, and avoid strenuous activity\",\n",
    "    \"Gender: F\\nCurrent Medications: Lisinopril 10mg daily, Furosemide 40mg daily\\nAllergies: None known\\nAssessment: Hypertension, fluid retention\",\n",
    "    \"Gender: M\\nChief Complaint: Fever, Cough\\nFindings: CXR shows consolidation in the right lower lobe\\nAssessment: Community-acquired pneumonia\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"Yes, the patient requires regular paracentesis due to fluid accumulation.\",\n",
    "    \"Yes, the patient requires close monitoring for fluid accumulation and symptoms.\",\n",
    "    \"Follow up with the doctor or nurse practitioner. Avoid heavy lifting and follow dietary guidelines.\",\n",
    "    \"The patient is currently taking Lisinopril and Furosemide.\",\n",
    "    \"The patient's primary diagnosis is community-acquired pneumonia.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3c806-2e03-4256-82e0-fbbc2a88f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt and validation functions\n",
    "def create_prompt(question, context):\n",
    "    \"\"\"Create a prompt for the model.\"\"\"\n",
    "    return f\"\"\"You are a clinical assistant. Provide concise, factual answers based ONLY on the available information.\n",
    "\n",
    "Question: {question}\n",
    "Available Context: {context if context.strip() else \"No specific clinical data provided\"}\n",
    "\n",
    "Answer (just the factual medical response, no references to tables/figures):\"\"\"\n",
    "\n",
    "def validate_answer(answer):\n",
    "    \"\"\"Validate generated answer to exclude invalid phrases.\"\"\"\n",
    "    invalid_phrases = [\"Table\", \"Figure\", \"as shown in\", \"refer to\"]\n",
    "    if any(phrase.lower() in answer.lower() for phrase in invalid_phrases):\n",
    "        return \"Unable to generate proper response from available data\"\n",
    "    return answer.strip()\n",
    "\n",
    "# Dataset class for generation\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset class for question answering.\"\"\"\n",
    "    def __init__(self, questions, inputs, references, tokenizer, max_length=256):\n",
    "        self.questions = questions\n",
    "        self.inputs = inputs\n",
    "        self.references = references\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        input_text = self.inputs[idx]\n",
    "        prompt = create_prompt(question, input_text)\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'prompt_length': encoding['input_ids'].shape[1],\n",
    "            'question': question,\n",
    "            'input_text': input_text,\n",
    "            'reference': self.references[idx]\n",
    "        }\n",
    "\n",
    "# Generate responses\n",
    "def generate_responses(model, tokenizer, questions, inputs, references):\n",
    "    \"\"\"Generate responses for the dataset.\"\"\"\n",
    "    bad_words = [\"Table\", \"Figure\"]\n",
    "    bad_words_ids = [tokenizer.encode(word, add_special_tokens=False) for word in bad_words if tokenizer.encode(word, add_special_tokens=False)]\n",
    "\n",
    "    generation_kwargs = {\n",
    "        'max_new_tokens': 150,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.3,\n",
    "        'repetition_penalty': 1.5,\n",
    "        'no_repeat_ngram_size': 4,\n",
    "        'bad_words_ids': bad_words_ids if bad_words_ids else None,\n",
    "        'eos_token_id': tokenizer.eos_token_id,\n",
    "        'pad_token_id': tokenizer.pad_token_id\n",
    "    }\n",
    "\n",
    "    qa_dataset = QADataset(questions, inputs, references, tokenizer)\n",
    "    dataloader = DataLoader(qa_dataset, batch_size=1, shuffle=False)\n",
    "    generated_outputs = []\n",
    "    sample_number = 0\n",
    "\n",
    "    try:\n",
    "        for batch in dataloader:\n",
    "            sample_number += 1\n",
    "            input_ids = batch['input_ids'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            attention_mask = batch['attention_mask'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            question = batch['question'][0]\n",
    "            input_text = batch['input_text'][0]\n",
    "            reference = batch['reference'][0]\n",
    "            prompt_length = batch['prompt_length'][0]\n",
    "\n",
    "            print(f\"\\n=== Sample {sample_number} ===\")\n",
    "            print(f\"Instruction: {question}\")\n",
    "            print(f\"Input: {input_text}\")\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.float16):\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "\n",
    "            if outputs.shape[1] > prompt_length:\n",
    "                new_tokens = outputs[0, prompt_length:]\n",
    "            else:\n",
    "                print(f\"Warning: No new tokens generated for sample {sample_number}\")\n",
    "                new_tokens = outputs[0]\n",
    "\n",
    "            generated_answer = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            generated_answer = validate_answer(generated_answer)\n",
    "\n",
    "            print(f\"Generated Answer: {generated_answer}\")\n",
    "            print(f\"Ground Truth Answer: {reference}\")\n",
    "\n",
    "            generated_outputs.append(generated_answer)\n",
    "\n",
    "        print(f\"\\nProcessed {sample_number} samples\")\n",
    "        return generated_outputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {str(e)}\")\n",
    "        print(f\"Stopped at sample {sample_number}\")\n",
    "        print(f\"Problematic sample details: {question}, {input_text}\")\n",
    "        return generated_outputs\n",
    "\n",
    "generated_outputs = generate_responses(model, tokenizer, questions, inputs, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673bce0-a379-4721-b236-86240afb18f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate generated outputs\n",
    "if generated_outputs:\n",
    "    compute_metrics_per_query(generated_outputs, references, questions, nlp, embedder)\n",
    "else:\n",
    "    print(\"No outputs generated due to error.\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32528124-1be3-46fd-962c-9ee46661976a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
