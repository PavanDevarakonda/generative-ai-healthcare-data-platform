{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHlPxroLy_uO",
    "outputId": "183ed896-3157-4415-d711-4ed9be0831f8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.cuda.get_device_name(0))  # Prints GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zm2HSb5TzAfZ",
    "outputId": "688ea2c4-4cee-4db7-c3b1-09b51f4c6c9b"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch pandas numpy scikit-learn rouge-score nltk scispacy\n",
    "\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy torch transformers bert-score rouge-score nltk spacy sentence-transformers mover-score meteor-score bart-score pyemd evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyemd pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhsddnH3zKys",
    "outputId": "4853c100-8344-4b49-dd65-b2c0db45f059"
   },
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y8sooV9S0cEC",
    "outputId": "0a960969-1c8a-4f73-864c-58352cf2a4d7"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy thinc spacy scispacy\n",
    "!pip install numpy==1.26.4\n",
    "!pip install spacy==3.7.2\n",
    "!pip install scispacy==0.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAkTnGfU5Pji",
    "outputId": "dfe8dee3-96e1-482b-a2c7-24f7377b489a"
   },
   "outputs": [],
   "source": [
    "!pip install  bert-score transformers torch sentence-transformers spacy nltk pandas rouge-score accelerate bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIHZbasiz5Yz",
    "outputId": "105129f1-51de-4fd0-dbc8-514fd90d4a50"
   },
   "outputs": [],
   "source": [
    "# Block 1: Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "import logging\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Suppress roberta-large warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Load spaCy and SentenceTransformer Models\n",
    "def load_spacy_model():\n",
    "    \"\"\"Load spaCy medical NER model.\"\"\"\n",
    "    return spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "def load_sentence_transformer():\n",
    "    \"\"\"Load SentenceTransformer for FCS.\"\"\"\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "\n",
    "nlp = load_spacy_model()\n",
    "embedder = load_sentence_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaBYaC6P1hBX"
   },
   "source": [
    "**Loading Dataset and creating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nC5AaoOT0BFI",
    "outputId": "6023f4e1-5654-4ee4-cefb-f801eb569e2a"
   },
   "outputs": [],
   "source": [
    "# âœ… Manually define 5 QA pairs (3 curated + 2 additional realistic)\n",
    "questions = [\n",
    "    \"Are there any further procedures planned for the patient?\",\n",
    "    \"Does the patient require long term monitoring?\",\n",
    "    \"What precautions does the patient need to take post-discharge?\",\n",
    "    \"What medications is the patient currently taking?\",\n",
    "    \"What is the patient's primary diagnosis?\"\n",
    "]\n",
    "\n",
    "inputs = [\n",
    "    \"Gender: F\\nChief Complaint: Abdominal distention, nausea, and vomiting\\nHistory: Cirrhosis, multiple paracenteses for ascites\\nPlan: Schedule regular paracentesis every 2 weeks\",\n",
    "    \"Gender: F\\nChief Complaint: Abdominal distention, nausea, and vomiting\\nPlan: Monitor weight and abdominal girth daily; assess for signs of fluid overload\",\n",
    "    \"Gender: M\\nChief Complaint: Abd pain, Hypotension\\nDischarge Plan: Follow low sodium diet, take prescribed meds, and avoid strenuous activity\",\n",
    "    \"Gender: F\\nCurrent Medications: Lisinopril 10mg daily, Furosemide 40mg daily\\nAllergies: None known\\nAssessment: Hypertension, fluid retention\",\n",
    "    \"Gender: M\\nChief Complaint: Fever, Cough\\nFindings: CXR shows consolidation in the right lower lobe\\nAssessment: Community-acquired pneumonia\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"Yes, the patient requires regular paracentesis due to fluid accumulation.\",\n",
    "    \"Yes, the patient requires close monitoring for fluid accumulation and symptoms.\",\n",
    "    \"Follow up with the doctor or nurse practitioner. Avoid heavy lifting and follow dietary guidelines.\",\n",
    "    \"The patient is currently taking Lisinopril and Furosemide.\",\n",
    "    \"The patient's primary diagnosis is community-acquired pneumonia.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN_ylBfP03C4"
   },
   "source": [
    "**Loading Biomistral model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1p3OOcQn05Jh",
    "outputId": "2b8a2a2c-f5a0-4b32-9bf1-d6ecb5056b48"
   },
   "outputs": [],
   "source": [
    "# Block 4: Load BioMistral 7B Model\n",
    "def load_model_and_tokenizer(model_name=\"BioMistral/BioMistral-7B\"):\n",
    "    \"\"\"Load BioMistral 7B model and tokenizer with quantization.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "print(f\"BioMistral-7B model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cInMJ-QyIDs0"
   },
   "source": [
    "**Response Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Prompt and Validation Functions\n",
    "def create_prompt(question, context):\n",
    "    \"\"\"Create a prompt for the model.\"\"\"\n",
    "    return f\"\"\"You are a clinical assistant. Provide concise, factual answers based ONLY on the available information.\n",
    "\n",
    "Question: {question}\n",
    "Available Context: {context if context.strip() else \"No specific clinical data provided\"}\n",
    "\n",
    "Answer (just the factual medical response, no references to tables/figures):\"\"\"\n",
    "\n",
    "def validate_answer(answer):\n",
    "    \"\"\"Validate generated answer to exclude invalid phrases.\"\"\"\n",
    "    invalid_phrases = [\"Table\", \"Figure\", \"as shown in\", \"refer to\"]\n",
    "    if any(phrase.lower() in answer.lower() for phrase in invalid_phrases):\n",
    "        return \"Unable to generate proper response from available data\"\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Dataset Class for Generation\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset class for question answering.\"\"\"\n",
    "    def __init__(self, questions, inputs, references, tokenizer, max_length=256):\n",
    "        self.questions = questions\n",
    "        self.inputs = inputs\n",
    "        self.references = references\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        input_text = self.inputs[idx]\n",
    "        prompt = create_prompt(question, input_text)\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'prompt_length': encoding['input_ids'].shape[1],\n",
    "            'question': question,\n",
    "            'input_text': input_text,\n",
    "            'reference': self.references[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyavPZ-n5jJ4",
    "outputId": "a31ec1bf-cc23-401c-c851-fba8e20fac25"
   },
   "outputs": [],
   "source": [
    "# Block 7: Generate Responses\n",
    "def generate_responses(model, tokenizer, questions, inputs, references):\n",
    "    \"\"\"Generate responses for the dataset.\"\"\"\n",
    "    bad_words = [\"Table\", \"Figure\"]\n",
    "    bad_words_ids = []\n",
    "    for word in bad_words:\n",
    "        encoded = tokenizer.encode(word, add_special_tokens=False)\n",
    "        if encoded:\n",
    "            bad_words_ids.append(encoded)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        'max_new_tokens': 150,\n",
    "        'do_sample': False,\n",
    "        'temperature': 0.3,\n",
    "        'repetition_penalty': 1.5,\n",
    "        'no_repeat_ngram_size': 4,\n",
    "        'bad_words_ids': bad_words_ids if bad_words_ids else None,\n",
    "        'eos_token_id': tokenizer.eos_token_id,\n",
    "        'pad_token_id': tokenizer.pad_token_id\n",
    "    }\n",
    "\n",
    "    qa_dataset = QADataset(questions, inputs, references, tokenizer)\n",
    "    dataloader = DataLoader(qa_dataset, batch_size=1, shuffle=False)\n",
    "    generated_outputs = []\n",
    "    sample_number = 0\n",
    "\n",
    "    try:\n",
    "        for batch in dataloader:\n",
    "            sample_number += 1\n",
    "            input_ids = batch['input_ids'].to(\"cuda\")\n",
    "            attention_mask = batch['attention_mask'].to(\"cuda\")\n",
    "            question = batch['question'][0]\n",
    "            input_text = batch['input_text'][0]\n",
    "            reference = batch['reference'][0]\n",
    "            prompt_length = batch['prompt_length'][0]\n",
    "\n",
    "            print(f\"\\n=== Sample {sample_number} ===\")\n",
    "            print(f\"Instruction: {question}\")\n",
    "            print(f\"Input: {input_text}\")\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "\n",
    "            if outputs.shape[1] > prompt_length:\n",
    "                new_tokens = outputs[0, prompt_length:]\n",
    "            else:\n",
    "                print(f\"Warning: No new tokens generated for sample {sample_number}\")\n",
    "                new_tokens = outputs[0]\n",
    "\n",
    "            generated_answer = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            generated_answer = validate_answer(generated_answer)\n",
    "\n",
    "            print(f\"Generated Answer: {generated_answer}\")\n",
    "            print(f\"Ground Truth Answer: {reference}\")\n",
    "\n",
    "            generated_outputs.append(generated_answer)\n",
    "\n",
    "        print(f\"\\nProcessed {sample_number} samples\")\n",
    "        return generated_outputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {str(e)}\")\n",
    "        print(f\"Stopped at sample {sample_number}\")\n",
    "        print(f\"Problematic sample details: {question}, {input_text}\")\n",
    "        return generated_outputs\n",
    "\n",
    "# Generate responses\n",
    "generated_outputs = generate_responses(model, tokenizer, questions, inputs, references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iJKGsOKIMBz"
   },
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uklKYEpgEM27"
   },
   "outputs": [],
   "source": [
    "# Block 9: Evaluate Generated Outputs\n",
    "if generated_outputs:\n",
    "    compute_metrics_per_query(generated_outputs, references, questions, nlp, embedder)\n",
    "else:\n",
    "    print(\"No outputs generated due to error.\")\n",
    "\n",
    "# Block 10: Clear GPU Memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
